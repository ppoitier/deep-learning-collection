{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-02T16:09:29.183737Z",
     "start_time": "2025-12-02T16:09:25.602408Z"
    }
   },
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch import nn\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "import lightning as pl\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "from dlc.trainers.img_classification import launch_training\n",
    "from dlc.vq_vae.model import VQVAE\n",
    "from dlc.trainers.vqvae import VQVAELightningModule"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T16:09:29.239358Z",
     "start_time": "2025-12-02T16:09:29.186844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "dataset = ImageFolder(root=\"D:/data/images/galaxy10_unamur/train\", transform=transform)\n",
    "train_dataset, test_dataset = random_split(dataset, lengths=(0.8, 0.2))\n",
    "print(\"#train samples:\", len(train_dataset))\n",
    "print(\"#test samples:\", len(test_dataset))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "print(\"#train batches:\", len(train_dataloader))\n",
    "print(\"#test batches:\", len(test_dataloader))"
   ],
   "id": "ec5598516ec2ec21",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train samples: 14691\n",
      "#test samples: 3672\n",
      "#train batches: 919\n",
      "#test batches: 230\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T16:09:29.246399Z",
     "start_time": "2025-12-02T16:09:29.242459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def display_sample(sample, unnormalize=True, ax=None):\n",
    "    ax = plt.gca() if ax is None else ax\n",
    "    img, label = sample\n",
    "    img = img * 0.5 + 0.5 if unnormalize else img\n",
    "    ax.set_title(dataset.classes[label])\n",
    "    ax.axis('off')\n",
    "    ax.imshow(img.permute(1, 2, 0).cpu().numpy())\n",
    "\n",
    "def display_batch(batch, unnormalize=True):\n",
    "    imgs, labels = batch\n",
    "    samples = random.sample(list(zip(imgs, labels)), k=8)\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    for i, sample in enumerate(samples):\n",
    "        plt.subplot(2, 4, i + 1)\n",
    "        display_sample(sample, unnormalize=unnormalize)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "b746af9632dc609",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T16:09:29.371252Z",
     "start_time": "2025-12-02T16:09:29.264414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vq_vae = VQVAE(\n",
    "    in_channels=3,\n",
    "    embedding_dim=256,\n",
    "    n_embeddings=512,\n",
    "    hidden_channels_enc=(64, 128, 256, 512, 512),\n",
    "    hidden_channels_dec=(512, 512, 256, 128, 64),\n",
    "    commitment_loss_factor=0.25,\n",
    "    quantization_loss_factor=1.0,\n",
    ")\n",
    "\n",
    "lightning_module = VQVAELightningModule(\n",
    "    vq_vae=vq_vae,\n",
    "    learning_rate=4e-4,\n",
    "    n_warmup_epochs=20,\n",
    "    plateau_patience=5,\n",
    "    plateau_factor=0.5,\n",
    ")"
   ],
   "id": "1c73be3c483c40d1",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T16:09:44.139515Z",
     "start_time": "2025-12-02T16:09:29.374559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lightning_trainer = pl.Trainer(\n",
    "    fast_dev_run=True,\n",
    "    max_epochs=500,\n",
    "    logger=TensorBoardLogger(save_dir=\"logs\"),\n",
    ")\n",
    "\n",
    "lightning_trainer.fit(\n",
    "    lightning_module,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=test_dataloader,\n",
    ")"
   ],
   "id": "8883acb103a58637",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "C:\\Users\\ppoitier\\miniforge3\\envs\\slp\\Lib\\site-packages\\torch\\__init__.py:1551: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\Context.cpp:85.)\n",
      "  return _C._get_float32_matmul_precision()\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                 | Type          | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | vq_vae               | VQVAE         | 31.0 M | train\n",
      "1 | train_loss           | MeanMetric    | 0      | train\n",
      "2 | val_loss             | MeanMetric    | 0      | train\n",
      "3 | train_recon_loss     | MeanMetric    | 0      | train\n",
      "4 | val_recon_loss       | MeanMetric    | 0      | train\n",
      "5 | train_quantizer_loss | MeanMetric    | 0      | train\n",
      "6 | val_quantizer_loss   | MeanMetric    | 0      | train\n",
      "7 | codebook_stats       | CodebookStats | 0      | train\n",
      "---------------------------------------------------------------\n",
      "31.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "31.0 M    Total params\n",
      "123.959   Total estimated model params size (MB)\n",
      "138       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "C:\\Users\\ppoitier\\miniforge3\\envs\\slp\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\ppoitier\\miniforge3\\envs\\slp\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e7483caa71c84d6080cd88fe0ccfe9ec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d8b3b2fe32e6458488a0f8dc1ed59b28"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
